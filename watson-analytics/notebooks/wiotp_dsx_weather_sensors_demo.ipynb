{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using IBM Data Science Experience to visualize data from Watson IoT Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This is an installation of spark-cloudant package, that may be missing. In general any missing python packages can be installed this way into your dsx account.\n",
    "This cell needs to be executed only once, restart the kernel afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pixiedust\n",
    "import pixiedust\n",
    "pixiedust.installPackage(\"cloudant-labs:spark-cloudant:2.0.0-s_2.11\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**User input required**\n",
    "\n",
    "Cloudant credentials.\n",
    "\n",
    "If you have a connection with Cloudant set up for this project, do the following:\n",
    "In order do import your Cloudant credentials, click on the cell below to select it. Then click on “Find and Add Data” button on the top right, select the Connections tab and click on “Insert to code”. A dictionary called credentials_1 should be added to the cell containing the credentials. If the dictionary has another name, change it to credentials_1. Then run the cell.\n",
    "If you don’t have a connection with Cloudant set up, the credantials can be found on Bluemix dashboard: Go to your Cloudant service on Bluemix, go to its Service Credentials section on the left and click on View Credentials to view the username and password. Set username and password variables below with Cloudant’s username and password."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This empty cell will be populated with your Cloudant credentials if you follow the steps explained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "username = credentials_1[\"username\"]\n",
    "password = credentials_1[\"password\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "host = username + '.cloudant.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**User input required**\n",
    "\n",
    "Cloudant database name.\n",
    "\n",
    "If you are not sure which database has the data you want to import, go to your Cloudant service on Bluemix and click on Launch. Find out the database name and set dbName variable with it. An example of database name is iotp_abcdef_default_2017_07_12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dbName = \"put your Cloudant database name here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to Cloudant database generated by WIoTP connector for hystorical data.\n",
    "\n",
    "The following piece of code connects to Cloudant NoSQL DB and returns an RDD data frame for the stored IoT data.\n",
    "The line `option(\"jsonstore.rdd.partitions\", 4)` is needed only if your Cloudant service plan is \"lite\" because this plan has an access quota of 5 requests per second. Spark may run parallel jobs that might lead to more than 5 requests being made in one second. If this happens, a \"too many requests\" error will be raised. If you experience this kind of error, reduce the value for the `jsonstore.rdd.partitions` option to 2. For paid service plans this line can be commented out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cloudantdata=sqlContext.read.format(\"com.cloudant.spark\").\\\n",
    "option(\"cloudant.host\", host).\\\n",
    "option(\"cloudant.username\", username).\\\n",
    "option(\"cloudant.password\", password).\\\n",
    "option(\"view\",\"_design/iotp/_view/by-date\").\\\n",
    "option(\"jsonstore.rdd.partitions\", 4).\\\n",
    "load(dbName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's observe the loaded data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cloudantdata.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the IOT data is located under the value column. \n",
    "Next we will transform this hierarchical  data frame into a flat one, and convert our timestamp from string into a timestamp type.\n",
    "\n",
    "The function withColumn adds a column named 'ts' to the data frame, and calculates it's content based on timestamp column (string), using the to_ts function we defined.\n",
    "\n",
    "The cache() function of a data frame caches the data frame in memory, this is very useful when data is accessed repeatedly.\n",
    "Most RDD operations are lazy. RDD operations that require observing the contents of the data cannot be lazy (These are called actions), and they cause the computation (of all previous lazy operations + the action) to be performed. The cache() operation causes the computation to be performed and cached for further usages, so that it is not performed repeatedly for every action.\n",
    "\n",
    "The weatherTelemetrics is a temporary view in the Spark Session. It will be useful for select statements (in case we choose write a raw SQL). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import udf, col, asc, desc,to_date, unix_timestamp, weekofyear, countDistinct\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import DateType, TimestampType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function converts the string cell into a timestamp type:\n",
    "str_to_ts =  udf (lambda d: datetime.strptime(d, \"%Y-%m-%dT%H:%M:%S.%fZ\"), TimestampType())\n",
    "\n",
    "sparkDf = cloudantdata.selectExpr(\"value.deviceId as deviceId\", \"value.deviceType as deviceType\", \"value.eventType as eventType\" ,  \"value.timestamp as timestamp\", \"value.data.*\")\n",
    "sparkDf = sparkDf.withColumn('ts', str_to_ts(col('timestamp')))\n",
    "sparkDf.cache()\n",
    "sparkDf.createOrReplaceTempView(\"weatherTelemetrics\")\n",
    "\n",
    "# show the resulting schema and data \n",
    "sparkDf.printSchema()\n",
    "spark.sql(\"SELECT * from weatherTelemetrics\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualization and comprehension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device Health \n",
    "\n",
    "In this section we will see how to learn about the population of IoT devices and answer questions such as: \n",
    "1. How many reports each device type had?\n",
    "2. What is the breakdown of the devices per device type?\n",
    "3. How many reports have been sent by each device? \n",
    "4. How many reports each event type had? \n",
    "5. How many devices reported in a given time interval?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from matplotlib import dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will make visualizations of the data.\n",
    "We will use Spark to prepare our data for visualization, since it supports big data processing.\n",
    "When the data is ready for visualization, we will convert Spark data Frame into Pandas data Frame, since Pandas has good visualization support. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many reports each device type had?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the deviceType as index of the created Pandas data frame will cause the bar plot to be aggregated by the deviceType.\n",
    "Next we call the plot function of the Pandas data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EperDtDF = spark.sql(\"SELECT ts,deviceType from weatherTelemetrics\").groupBy(\"deviceType\").count()\n",
    "EperDtDF.cache()\n",
    "EperDtDF.show()\n",
    "\n",
    "EperDtPanda = EperDtDF.toPandas().set_index('deviceType')\n",
    "\n",
    "ax = EperDtPanda.plot(kind='bar',legend=False)\n",
    "ax.set_xlabel(\"deviceType\")\n",
    "ax.set_ylabel(\"events count\")\n",
    "ax.set_title('count of events by deviceType')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the breakdown of the devices per device type?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bar chart is plotted in the same way, as before, but now we will also show the pie chart of the data.\n",
    "Pandas data frame support all kinds of different plot types. Using the 'pie' kind will make a pie chart with percentage sizes of the pieces. \n",
    "In order to write the actual count of the devices, instead of percents, we use the autopct argument - we need to multiply by the total amount of devices we have and divide by 100 to get the actual numbers.\n",
    "The total is calculated using the sum() function of Pandas data frame, that sums up the device count of all the deviceTypes.\n",
    "The sum function returns a DataFrame, so we use the [0] index to get only the value into total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DperDtDF = spark.sql(\"SELECT deviceId,deviceType from weatherTelemetrics\").groupBy(\"deviceType\").agg(countDistinct('deviceId'))\n",
    "EperDtDF.cache()\n",
    "DperDtDF.show()\n",
    "\n",
    "# bar chart of deviceId by deviceType\n",
    "EperDtPanda = DperDtDF.toPandas().set_index('deviceType')\n",
    "\n",
    "ax = EperDtPanda.plot(kind='bar',legend=False)\n",
    "ax.set_xlabel(\"deviceType\")\n",
    "ax.set_ylabel(\"devices count\")\n",
    "ax.set_title('count of deviceIds by deviceType')\n",
    "\n",
    "\n",
    "# Pie chart of deviceId by deviceType\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "ax = plt.subplot(111)\n",
    "total = EperDtPanda.sum()[0]\n",
    "\n",
    "ax = EperDtPanda.plot(kind='pie', ax=ax, figsize=(5,5), legend=False, shadow=True, subplots=True, autopct=lambda(p): '{:.0f}'.format(p * total / 100))\n",
    "plt.title(\"count of deviceIds by deviceType\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many reports have been sent by each device? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EperDdf = spark.sql(\"SELECT deviceId,ts from weatherTelemetrics\").groupBy(\"deviceId\").count()####.sort()########\n",
    "EperDtDF.cache()\n",
    "EperDdf.show()\n",
    "\n",
    "EperDPanda = EperDdf.toPandas().set_index('deviceId')\n",
    "\n",
    "ax = EperDPanda.plot(kind='bar',legend=False)\n",
    "ax.set_xlabel(\"deviceId\")\n",
    "ax.set_ylabel(\"events count\")\n",
    "ax.set_title('count of events by deviceId')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many reports each event type had? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EperEtDF = spark.sql(\"SELECT eventType,ts from weatherTelemetrics\").groupBy(\"eventType\").count()\n",
    "EperDtDF.cache()\n",
    "EperEtDF.show()\n",
    "\n",
    "EperEtPanda = EperEtDF.toPandas().set_index('eventType')\n",
    "\n",
    "ax = EperEtPanda.plot(kind='bar',legend=False)\n",
    "ax.set_xlabel(\"eventType\")\n",
    "ax.set_ylabel(\"events count\")\n",
    "ax.set_title('count of events by eventType')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### how many devices reported in a given time interval?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**User input required**\n",
    "\n",
    "Replace the (year, month, day, hours, minutes, seconds) with values to specify `start` and `end` interval in the cell below.\n",
    "\n",
    "For example:\n",
    "\n",
    "`start = datetime(2017, 7, 12, 21, 35, 0)\n",
    "end = datetime(2017, 7, 12, 21, 35, 3)`\n",
    "\n",
    "Make sure the interval contains device events. Then run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set the time interval of interest\n",
    "start = datetime(year, month, day, hours, minutes, seconds)\n",
    "end = datetime(year, month, day, hours, minutes, seconds) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we filter the data by a time interval, then group the resulting dataFrame by deviceId, and count the records for each deviceId."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#filter by time interval\n",
    "weatherMetaDataTelemetrics = sparkDf.select('deviceId','deviceType','ts','timestamp','eventType').filter((col('ts')>=start) & (col('ts')<=end))\n",
    "\n",
    "weatherMetaDataTelemetrics.cache()\n",
    "#weatherMetaDataTelemetrics.show()\n",
    "\n",
    "#how many devices reported in interval\n",
    "byDevice = weatherMetaDataTelemetrics.groupby(['deviceId']).count()\n",
    "byDevice.cache()\n",
    "\n",
    "print \"Number of events by deviceId in the interval: \"\n",
    "byDevice.show()\n",
    "print \"total number of devices reported in the interval:\", byDevice.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count of rows by time span for a specific device, using the filter function of Spark DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "byDevice.filter(byDevice[\"deviceId\"]=='66666666').show() ##also show 5 with lowest counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract all of the numeric columns of the data, for further analytics. I selected only a subset of the numeric columns, for this demonstration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#find all numeric columns of the DataFrame\n",
    "numericCols = filter( lambda(name, dt) : (('double' in dt) or ('int' in dt) or ('long' in dt)), sparkDf.dtypes) \n",
    "\n",
    "#numericCols is a list of pairs (columnName, dataType), here we select only the column name into the allkeys list\n",
    "allkeys = [x[0] for x in numericCols]\n",
    "print \"all numeric columns\", allkeys\n",
    "\n",
    "#select only 5 numeric columns for further detailed visualizations\n",
    "keys = ['O3', 'NOX', 'NO2', 'PM10', 'PM2_5']\n",
    "print \"selected 5 numeric columns\", keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Device type sensor visualization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we study the summary of sensor data reported by all devices of a device type, answering questions such as: \n",
    "\n",
    "1. What is the Average/Min/Max of all reported sensor values? \n",
    "2. Can I see a histogram of a sensor's output?   \n",
    "3. What is the correlation between two sensors?\n",
    "\n",
    " ***add the visualization that are not stated here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average/Min/Max of all reported sensor values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "#show visualization for device type \"dt1\"\n",
    "dType = \"dt1\"\n",
    "\n",
    "#show summary only for the selected 5 columns, for easier view, since we have too many columns to fit in a row\n",
    "dfKeysType = sparkDf.select(*keys).filter(sparkDf[\"deviceType\"]==dType)\n",
    "dfKeysType.cache()\n",
    "\n",
    "dfKeysType.describe().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogram of a device type sensor's output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use spark DataFrame to prepare the histogram for each specific sensor (key) (using rdd.flatMap)\n",
    "2. Create Pandas DataFrame from the calculated histogram with 2 columns: \"bin\" and \"frequency\".\n",
    "3. Plot the histogram using Pandas plot function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for key in keys:\n",
    "    histogram = dfKeysType.select(key).rdd.flatMap(lambda x: x).histogram(11)\n",
    "    \n",
    "    #print histogram\n",
    "    pandaDf = pd.DataFrame(zip(list(histogram)[0],list(histogram)[1]),columns=['bin','frequency']).set_index('bin')\n",
    "    ax =pandaDf.plot(kind='bar')\n",
    "    ax.set_ylabel(\"frequency\")\n",
    "    ax.set_title('Histogram of ' + key + ' sensor output')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation between two sensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation between two sensors can be plotted using Pandas plot with kind='scatter'.\n",
    "Remeber that dfKeysType is a Spark DataFrame that only includes our selected 5 columns and filtered by deviceType.\n",
    "In case this will result in too much data for Pandas DataFrame to handle, it can be further filtered by timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "key1=\"PM10\"\n",
    "key2=\"PM2_5\"\n",
    "\n",
    "pandaDF = dfKeysType.toPandas()\n",
    "ax = pandaDF.plot(kind='scatter', x=key1, y=key2, s=5, figsize=(7,7))\n",
    "ax.set_title('Relationship between ' + key1 + ' and ' + key2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view all the correlations of the selected 5 columns, together with a histogram on a diagonal, use the Pandas scatter_matrix function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.scatter_matrix(pandaDF, figsize=(18,12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A correlation matrix can be plotted, using Pandas corr() function on the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correlations = pandaDF.corr()\n",
    "\n",
    "# plot correlation matrix\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(correlations, vmin=-1, vmax=1)\n",
    "fig.colorbar(cax)\n",
    "ticks = np.arange(0,5,1)\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_yticks(ticks)\n",
    "ax.set_xticklabels(keys)\n",
    "ax.set_yticklabels(keys)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensor deep dive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we produce much of the same charts, as in previous section, but filtering the data by specific deviceId."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average/Min/Max of all reported sensor values by the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#show visualization for specific deviceID\n",
    "deviceId = \"10101010\"\n",
    "\n",
    "#show summary only for a selected group of columns, for easier view, since we have too many columns to fit in a row\n",
    "dfKeysDev = sparkDf.select(*keys).filter(sparkDf[\"deviceId\"]==deviceId)\n",
    "dfKeysDev.cache()\n",
    "\n",
    "dfKeysDev.describe().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A box plot is a method for graphically depicting groups of numerical data through their quartiles.\n",
    "The box extends from the lower to upper quartile values of the data, with a line at the median. \n",
    "The whiskers extend from the box to show the range of the data. \n",
    "Beyond the whiskers, data are considered outliers and are plotted as individual points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A box plot for each devices sensor, produced with the Pandas plot function with kind=\"box\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pandaDF = dfKeysDev.toPandas()\n",
    "pandaDF.plot(kind='box', subplots=True, layout=(10,3), sharex=False, sharey=False, figsize=(25,60))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Histogram of a device's sensor output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for key in keys:\n",
    "    try:\n",
    "        #The histogram is built with spark. Only the groupped by bins data will be converted to Pandas DataFrame\n",
    "        histogram = dfKeysDev.select(key).rdd.flatMap(lambda x: x).histogram(11)\n",
    "\n",
    "        #print histogram\n",
    "        pandaDf = pd.DataFrame(zip(list(histogram)[0],list(histogram)[1]),columns=['bin','frequency']).set_index('bin')\n",
    "        ax = pandaDf.plot(kind='bar')\n",
    "        ax.set_ylabel(\"frequency\")\n",
    "        ax.set_title('Histogram of ' + key + ' sensor output')\n",
    "   \n",
    "    except: \n",
    "        print \"no values for sensor \" + key + \" for device \" + deviceId + \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histograms can also be built more easily with Pandas DataFrame, in case the dfKeysDev DataFrame is not too large.\n",
    "For the case of big data, spark is more scalable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pandaDF = dfKeysDev.toPandas()\n",
    "pandaDF.hist(layout=(3,3), sharex=False, figsize=(20,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Density Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel density estimation (KDE) is a non-parametric way to estimate the probability density function of a random variable. Kernel density estimation is a fundamental data smoothing problem where inferences about the population are made, based on a finite data sample.\n",
    "\n",
    "Note: here we convert the data into Pandas DataFrame, after we filtered it by deviceId and selected a subset of keys.\n",
    "In case this is still to much data for the Pandas DataFrame to handle, consider selecting fewer keys and filtering by time interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pandaDF = dfKeysDev.toPandas()\n",
    "\n",
    "ax = pandaDF.plot(kind='density', subplots=True, layout=(3,3), sharex=False, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How a specific device sensor value changes over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum, minimum and average lines are shown on plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We can aggregate data by intervals (say hour) in spark and show aggregated plots (avg, min, max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, min, max\n",
    "\n",
    "#show visualization for specific deviceID\n",
    "deviceId = \"10101010\"\n",
    "\n",
    "print keys\n",
    "for key in keys:\n",
    "    df = spark.sql(\"SELECT deviceId, ts,\" + key +\" from weatherTelemetrics where deviceId='\" + deviceId + \"'\").where(col(key).isNotNull())\n",
    "    df.cache()\n",
    "    if (df.count() > 0):\n",
    "        pandaDF = df.toPandas()\n",
    "        \n",
    "        ax = pandaDF.plot(x='ts', y=key , legend=False, figsize=(15,9), ls='-', marker='o')\n",
    "        ax.xaxis.set_major_formatter(dates.DateFormatter('%d-%m-%Y %H:%M:%S'))\n",
    "        ax.set_title(key + ' over time')\n",
    "        ax.set_ylabel(key)\n",
    "        ax.grid(True)\n",
    "        \n",
    "        # Draw lines to showcase the upper and lower threshold\n",
    "        ax.axhline(y=pandaDF[key].min(),c=\"red\",linewidth=2,zorder=0)\n",
    "        ax.axhline(y=pandaDF[key].max(),c=\"red\",linewidth=2,zorder=0)\n",
    "        ax.axhline(y=pandaDF[key].mean(),c=\"green\",linewidth=2,zorder=0, ls='--')\n",
    "    \n",
    "        ax.autoscale_view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare between the sensor values of devices over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the dfKeysDev DataFrame contains only keys columns, with no ts column, so we will create a new data frame that will also include the ts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#show visualization for specific deviceID\n",
    "deviceId = \"10101010\"\n",
    "\n",
    "columns = list(keys)\n",
    "columns.append('ts')\n",
    "df = sparkDf.select(*columns).filter(sparkDf[\"deviceId\"]==deviceId)\n",
    "\n",
    "pandaDF = df.toPandas().set_index('ts')\n",
    "ax = pandaDF.plot(figsize=(15,9),ls='', marker='o')   \n",
    "ax.xaxis.set_major_formatter(dates.DateFormatter('%d-%m-%Y %H:%M:%S'))\n",
    "ax.set_title(', '.join(keys) + ' over time')\n",
    "ax.grid(True)\n",
    "ax.autoscale_view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pandaDF = dfKeysDev.toPandas()\n",
    "\n",
    "pd.scatter_matrix(pandaDF, figsize=(18,12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z-score\n",
    "\n",
    "Z-score is a standard score that indicates how many standard deviations an element is from the mean.\n",
    "\n",
    "A z-score can be calculated from the following formula \n",
    "\n",
    "z = (X - µ) / σ  \n",
    "\n",
    "where z is the z-score, X is the value of the element, µ is the population mean, and σ is the standard deviation\n",
    "\n",
    "A higher z-score value represents a larger deviation from the mean value which can be interpreted as abnormal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will calculate z-score for each selected column (sensor) of each deviceId, and plot only the sensors that have spikes.\n",
    "We define a spike in the below function spike(row), by reported value having z-score above 3 or below -3.\n",
    "Observe that the values for which the z-score is above 3 or below -3, marked as abnormal events in the graph below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ignore warnings if any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "'''\n",
    "This function detects the spike and dip by returning a non-zero value \n",
    "when the z-score is above 3 (spike) and below -3(dip). Incase if you \n",
    "want to capture the smaller spikes and dips, lower the zscore value from \n",
    "3 to 2 in this function.\n",
    "'''\n",
    "upperThreshold = 3\n",
    "lowerThreshold = -3\n",
    "def spike(row):\n",
    "    if(row['zscore'] >=upperThreshold or row['zscore'] <=lowerThreshold):\n",
    "        return row[key]\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, min, max, mean, stddev\n",
    "\n",
    "#get the list of available devices\n",
    "devices = sparkDf.select(\"deviceId\").groupBy(\"deviceId\").count().rdd.map(lambda r: r[0]).collect()\n",
    "\n",
    "#calculate for each device and each key\n",
    "for dev in devices:\n",
    "    for key in allkeys:\n",
    "        df = spark.sql(\"SELECT deviceId, ts,\" + key +\" from weatherTelemetrics where deviceId='\" + dev + \"'\").where(col(key).isNotNull())\n",
    "        if (df.count() > 0):\n",
    "            pandaDF = df.toPandas().set_index(\"ts\")\n",
    "            \n",
    "            # calculate z-score and populate a new column\n",
    "            pandaDF['zscore'] = (pandaDF[key] - pandaDF[key].mean())/pandaDF[key].std(ddof=0)\n",
    "\n",
    "            #add new column - spike, and calculate its value based on the thresholds, usinf spike function, defined above\n",
    "            pandaDF['spike'] = pandaDF.apply(spike, axis=1)\n",
    "            \n",
    "            \n",
    "            #plot the chart, only if spikes were detected (not all values of \"spike\" are zero)\n",
    "            if (pandaDF['spike'].nunique() > 1):\n",
    "                # select rows that are required for plotting\n",
    "                plotDF = pandaDF[[key,'spike']]\n",
    "                #calculate the y minimum value\n",
    "                y_min = (pandaDF[key].max() - pandaDF[key].min()) / 10\n",
    "                fig, ax = plt.subplots(num=None, figsize=(14, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "                ax.set_ylim(plotDF[key].min() - y_min, plotDF[key].max() + y_min)\n",
    "                x_filt = plotDF.index[plotDF.spike != 0]\n",
    "                plotDF['spikes'] = plotDF[key]\n",
    "                y_filt = plotDF.spikes[plotDF.spike != 0]\n",
    "                #Plot the raw data in blue colour\n",
    "                line1 = ax.plot(plotDF.index, plotDF[key], '-', color='blue', animated = True, linewidth=1, marker='o')\n",
    "                #plot the anomalies in red circle\n",
    "                line2 = ax.plot(x_filt, y_filt, 'ro', color='red', linewidth=2, animated = True)\n",
    "                #Fill the raw area\n",
    "                ax.fill_between(plotDF.index, (pandaDF[key].min() - y_min), plotDF[key], interpolate=True, color='blue',alpha=0.6)\n",
    "\n",
    "                # calculate the sensor value that is corresponding to z-score that defines a spike\n",
    "                valUpperThreshold = (pandaDF[key].std(ddof=0) * upperThreshold) + pandaDF[key].mean()\n",
    "                # calculate the sensor value that is corresponding to z-score that defines a dip\n",
    "                valLowerThreshold = (pandaDF[key].std(ddof=0) * lowerThreshold) + pandaDF[key].mean()\n",
    "\n",
    "                #plot the thresholds\n",
    "                ax.axhline(y=valUpperThreshold,c=\"red\",linewidth=2,zorder=0,linestyle='dashed',label='Upper threshold')\n",
    "                ax.axhline(y=valLowerThreshold,c=\"red\",linewidth=2,zorder=0,linestyle='dotted',label='Lower threshold')\n",
    "                \n",
    "                # Label the axis\n",
    "                ax.set_xlabel(\"Sequence\",fontsize=20)\n",
    "                ax.set_ylabel(key,fontsize=20)\n",
    "                ax.set_title(\"deviceId: \" + dev + \" sensor:\" + key)\n",
    "                plt.tight_layout()\n",
    "                plt.legend()\n",
    "        \n",
    "                print \"sensor value that corresponds to z-score \" , upperThreshold , \": \" , valUpperThreshold\n",
    "                print \"sensor value that corresponds to z-score \", lowerThreshold, \": \" , valLowerThreshold\n",
    "                \n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As shown, the red marks are the unexpected spikes and of whose z-score value is greater than 3 or less than -3. Incase if you want to detect the lower spikes, modify the value to 2 or even lower and run. Similarly, if you want to detect only the higher spikes, try increasing the z-score value from 3 to 4 and beyond."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 2.1",
   "language": "python",
   "name": "python2-spark21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
